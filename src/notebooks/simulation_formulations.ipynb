{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# Advanced Training Metrics and Formulations\n",
    "\n",
    "This notebook provides a detailed walkthrough of the key metrics and mathematical formulations used in our ensemble training pipeline. In this notebook, we cover:\n",
    "\n",
    "- **Token‑Level and Sequence‑Level Entropy:** Measuring uncertainty in model predictions.\n",
    "- **Logistic Regression Ensemble:** Combining features such as model confidences, entropy, and KNN similarity to generate a final quality score.\n",
    "- **FAISS‑based KNN Retrieval:** Fast retrieval of similar examples using normalized embeddings and cosine similarity.\n",
    "- **Cross‑Model Alignment Loss:** Optimizing the shared latent space between models with adversarial perturbation.\n",
    "- **Knowledge Distillation:** Training a compact student model to mimic the ensemble output.\n",
    "\n",
    "The notebook also includes visualizations of training loss and entropy trends over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For LaTeX rendering in markdown cells\n",
    "from IPython.display import display, Math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entropy-math",
   "metadata": {},
   "source": [
    "## Entropy Calculations\n",
    "\n",
    "For a probability distribution \\(\\mathbf{p}\\), the token-level entropy is defined as:\n",
    "\n",
    "$$ H(\\mathbf{p}) = -\\sum_{i} p_i \\log p_i $$\n",
    "\n",
    "This measure is then aggregated over the sequence. For example, the mean entropy over a sequence of \\(n\\) tokens is:\n",
    "\n",
    "$$ H_{seq} = \\frac{1}{n} \\sum_{i=1}^{n} H(p_i) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entropy-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_entropy(logits, temperature=1.0):\n",
    "    \"\"\"Calculate token-level entropy from logits with temperature scaling.\"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    log_probs = torch.log(probs + 1e-10)\n",
    "    token_entropy = -torch.sum(probs * log_probs, dim=-1)\n",
    "    return token_entropy\n",
    "\n",
    "def aggregate_sequence_entropy(token_entropies, reduction='mean'):\n",
    "    \"\"\"Aggregate token-level entropy to a single sequence-level value.\"\"\"\n",
    "    if reduction == 'mean':\n",
    "        return token_entropies.mean()\n",
    "    elif reduction == 'max':\n",
    "        return token_entropies.max()\n",
    "    elif reduction == 'weighted_mean':\n",
    "        weights = torch.arange(1, token_entropies.size(0) + 1).float() / token_entropies.size(0)\n",
    "        return (token_entropies * weights).sum() / weights.sum()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reduction method: {reduction}\")\n",
    "\n",
    "# Simulate logits for demonstration\n",
    "dummy_logits = torch.randn(5, 100)  # Batch size 5, sequence length 100\n",
    "token_entropy = calculate_token_entropy(dummy_logits)\n",
    "sequence_entropy = aggregate_sequence_entropy(token_entropy)\n",
    "print('Mean Sequence Entropy:', sequence_entropy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logreg-math",
   "metadata": {},
   "source": [
    "## Logistic Regression Ensemble\n",
    "\n",
    "The ensemble logistic regression layer fuses multiple features to output a final quality score. Given the feature vector \\( \\mathbf{x} \\in \\mathbb{R}^7 \\), the logistic regression prediction is defined as:\n",
    "\n",
    "$$ \\hat{y} = \\sigma(\\mathbf{w}^T \\mathbf{x} + b) $$\n",
    "\n",
    "where \\( \\sigma(z) = \\frac{1}{1+e^{-z}} \\) is the sigmoid function. The features include:\n",
    "\n",
    "- **f1:** Primary confidence (\\(1 - H_{seq}\\))\n",
    "- **f2:** Secondary confidence (based on ensemble disagreement)\n",
    "- **f3:** Evaluator confidence (\\(1 - \\) evaluator entropy)\n",
    "- **f4:** Raw primary sequence entropy\n",
    "- **f5:** Ensemble disagreement\n",
    "- **f6:** KNN similarity score\n",
    "- **f7:** Example quality metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logreg-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EnsembleLogisticRegression(nn.Module):\n",
    "    def __init__(self, feature_dim=7):\n",
    "        super(EnsembleLogisticRegression, self).__init__()\n",
    "        self.logistic = nn.Linear(feature_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, features):\n",
    "        logits = self.logistic(features)\n",
    "        return self.sigmoid(logits)\n",
    "\n",
    "# Simulate a feature vector\n",
    "features = torch.tensor([[0.8, 0.7, 0.9, 0.2, 0.3, 0.85, 1.0]])\n",
    "ensemble_lr = EnsembleLogisticRegression()\n",
    "score = ensemble_lr(features)\n",
    "print('Ensemble Prediction Score:', score.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knn-math",
   "metadata": {},
   "source": [
    "## FAISS-based KNN Retrieval\n",
    "\n",
    "To quickly retrieve similar code examples, we use a FAISS-based index. The cosine similarity between normalized embeddings \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is computed as:\n",
    "\n",
    "$$ \\text{similarity}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|} $$\n",
    "\n",
    "High similarity indicates that the retrieved example is very similar to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class KNNIndex:\n",
    "    def __init__(self, embedding_dim):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index = faiss.IndexFlatIP(embedding_dim)  # Inner product for cosine similarity\n",
    "        self.examples = []\n",
    "        self.metadata = []\n",
    "    def add_example(self, text, embedding, metadata=None):\n",
    "        if embedding.shape[0] != self.embedding_dim:\n",
    "            raise ValueError(f\"Expected embedding dimension {self.embedding_dim}, got {embedding.shape[0]}\")\n",
    "        self.index.add(np.array([embedding]).astype('float32'))\n",
    "        self.examples.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    def search(self, query_embedding, k=5):\n",
    "        query_embedding = np.array([query_embedding]).astype('float32')\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                results.append({\n",
    "                    'text': self.examples[idx],\n",
    "                    'distance': distances[0][i],\n",
    "                    'metadata': self.metadata[idx]\n",
    "                })\n",
    "        return results\n",
    "\n",
    "# Simulate adding and retrieving examples\n",
    "knn = KNNIndex(embedding_dim=384)\n",
    "example_embedding = np.random.rand(384).astype('float32')\n",
    "knn.add_example(\"def example(): pass\", example_embedding, {\"quality\": 0.95})\n",
    "results = knn.search(example_embedding, k=1)\n",
    "print('KNN Retrieval Results:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alignment-math",
   "metadata": {},
   "source": [
    "## Cross‑Model Alignment Loss\n",
    "\n",
    "To align the latent spaces of different models, we project their final hidden states into a common space and compute cosine similarity. The alignment loss is computed as the average of the disagreements:\n",
    "\n",
    "$$ \\mathcal{L}_{align} = \\frac{1}{3} \\Bigl( (1 - \\cos(\\mathbf{z}_1, \\mathbf{z}_2)) + (1 - \\cos(\\mathbf{z}_1, \\mathbf{z}_3)) + (1 - \\cos(\\mathbf{z}_2, \\mathbf{z}_3)) \\Bigr) $$\n",
    "\n",
    "where \\( \\mathbf{z}_1, \\mathbf{z}_2, \\mathbf{z}_3 \\) are the projected representations of the primary, secondary, and evaluator models respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alignment-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell would normally run the cross-model alignment in the training loop.\n",
    "# Here we simulate random projected vectors for demonstration:\n",
    "z1 = F.normalize(torch.randn(1, 512), p=2, dim=1)\n",
    "z2 = F.normalize(torch.randn(1, 512), p=2, dim=1)\n",
    "z3 = F.normalize(torch.randn(1, 512), p=2, dim=1)\n",
    "\n",
    "cos12 = F.cosine_similarity(z1, z2).item()\n",
    "cos13 = F.cosine_similarity(z1, z3).item()\n",
    "cos23 = F.cosine_similarity(z2, z3).item()\n",
    "\n",
    "alignment_loss = ( (1 - cos12) + (1 - cos13) + (1 - cos23) ) / 3.0\n",
    "print(f\"Alignment Loss: {alignment_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distillation-math",
   "metadata": {},
   "source": [
    "## Advanced Knowledge Distillation\n",
    "\n",
    "In our system, a student MLP is trained to mimic the output of the ensemble projection (e.g., the output from the primary model’s projection layer). The training objective is to minimize the MSE loss between the student’s prediction and the target projection:\n",
    "\n",
    "$$ \\mathcal{L}_{distill} = \\| f_{student}(\\mathbf{h}) - f_{target}(\\mathbf{h}) \\|^2 $$\n",
    "\n",
    "where \\( \\mathbf{h} \\) represents the hidden states from the primary model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-notes",
   "metadata": {},
   "source": [
    "## Final Visualization\n",
    "\n",
    "Below we visualize simulated training loss and entropy metrics over epochs to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training metrics\n",
    "epochs = np.arange(1, 11)\n",
    "loss = np.exp(-epochs / 3) + np.random.normal(0, 0.05, size=10)\n",
    "entropy = np.linspace(4.5, 3.0, 10) + np.random.normal(0, 0.1, size=10)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax[0].plot(epochs, loss, marker='o')\n",
    "ax[0].set_title('Simulated Training Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "\n",
    "ax[1].plot(epochs, entropy, marker='o', color='orange')\n",
    "ax[1].set_title('Simulated Sequence Entropy')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Entropy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
